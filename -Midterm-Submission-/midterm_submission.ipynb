{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 : Multi-source Financial News Collection\n",
    "import requests \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "def fetch_news(source,url,articles):\n",
    "    response=requests.get(url)\n",
    "    if response.status_code!=200:\n",
    "        print(\"failed to retrieve 'source' news\")\n",
    "        return []\n",
    "    soup=BeautifulSoup(response.text,\"xml\")\n",
    "    items=soup.find_all(\"item\")\n",
    "    for item in items:\n",
    "        headline= item.find(\"title\")\n",
    "        pub_Date=item.find(\"pubDate\")\n",
    "        articles.append({\n",
    "            \"source\":source,\n",
    "            \"headline\":headline.text if headline else \"N/A\",\n",
    "            \"pub_Date\":pub_Date.text if pub_Date else \"N/A\"\n",
    "        })\n",
    "\n",
    "    \n",
    "rss_feeds={\n",
    "    'Wall Street Journal':'https://feeds.content.dowjones.io/public/rss/RSSUSnews',\n",
    "    'Bloomberg':'https://feeds.bloomberg.com/markets/news.rss',\n",
    "    'MarketWatch':'https://feeds.content.dowjones.io/public/rss/mw_marketpulse'\n",
    "} \n",
    "articles=[]\n",
    "fetch_news('Wall Street Journal','https://feeds.content.dowjones.io/public/rss/RSSUSnews',articles)\n",
    "fetch_news('MarketWatch','https://feeds.content.dowjones.io/public/rss/mw_marketpulse',articles)\n",
    "fetch_news('Bloomberg','https://feeds.bloomberg.com/markets/news.rss',articles)\n",
    "df=pd.DataFrame(articles)\n",
    "df.to_csv(r\"c:\\Users\\Amol Natu\\Wids Project\\news_raw.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35961f",
   "metadata": {},
   "source": [
    "#Task 2: XML Structure Understanding\n",
    "\n",
    "##Which XML tags were used to extract the headlines?\n",
    "The XML tags which were used to extract headlines were <item>,<title> and <pubDate>.\n",
    "\n",
    "##What is the role of the <item> tag in RSS feeds?\n",
    "The <item> tag in RSS feeds represents a single news headline. It marks the start of a new news article.\n",
    "\n",
    "##How does an RSS feed differ from a normal HTML webpage?\n",
    "An RSS feed is designed and written for machine readability while a normal HTML webpage is designed to be read by humans in a readable and systematic visual format.RSS feeds provide standardized tags that make it easy to extract specific information using a program,while HTML pages require web scraping and are less consistent in structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 : News Data Cleaning and Standardization\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "news_df=pd.read_csv('news_raw.csv') #read the csv file \n",
    "pd.set_option(\"display.max_colwidth\", None) #show full column content\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "news_df['pub_Date']=pd.to_datetime(news_df['pub_Date'],utc=True) #convert to a datetime object \n",
    "news_df[\"date\"]=news_df[\"pub_Date\"].dt.date #extract date from a datetime object\n",
    "news_df[\"headline_length\"]=news_df[\"headline\"].str.len() #calculate headline length\n",
    "news_df.to_csv(\"news_cleaned.csv\",index=False) #save the cleaned data to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 : Stock Price Data Collection\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import yfinance as yf \n",
    "ticker=\"TSLA\"\n",
    "stock_df=yf.download(ticker,period=\"10d\",interval=\"1d\")\n",
    "stock_df.to_csv(\"stock_data.csv\",index=True)\n",
    "print (stock_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06ed37",
   "metadata": {},
   "source": [
    "# Task 5: Market Calendar Awareness \n",
    "\n",
    "# Which dates in your news data are non-trading days?\n",
    "Based on the News in the dataset the Days which are no n trading days are:\n",
    "01-01-2026-(New Year's Day(Market Holiday))\n",
    "28-12-2025-(Sunday)\n",
    "27-12-2025-(Saturday)\n",
    "21-12-2025-(Sunday)\n",
    "20-12-2025-(Saturday)\n",
    "28-12-2024-(Saturday)\n",
    "\n",
    "# Why does the stock market not trade on those days?\n",
    "The U.S. stock market does not operate on:\n",
    "Weekends, because financial exchanges such as the NYSE and NASDAQ are officially closed on Saturdays and Sundays.\n",
    "Federal holidays, such as New Yearâ€™s Day (January 1st), when markets are closed to observe national holidays.\n",
    "On these days, no official stock price data is generated.\n",
    "\n",
    "# How many news articles fall on non-trading days?\n",
    "10 news articles were published on weekends.\n",
    "18 news articles were published on New Year's Day.\n",
    "Total 28 news articles were published on non-Trading Days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43dce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6 : Intelligent Data Merging\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "news_df=pd.read_csv(\"news_cleaned.csv\")\n",
    "stock_df=pd.read_csv(\"stock_data.csv\",skiprows=2)\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"], format=\"%d-%m-%Y\")\n",
    "news_df[\"date\"] = pd.to_datetime(news_df[\"date\"], format=\"%d-%m-%Y\")\n",
    "merged_df=pd.merge(\n",
    "    news_df,\n",
    "    stock_df,\n",
    "    left_on=\"date\",\n",
    "    right_on=\"Date\",\n",
    "   how=\"left\"\n",
    ")\n",
    "merged_df[\"is_trading_day\"] = merged_df[\"Date\"].notna()\n",
    "merged_df=merged_df.rename(columns={\"Unnamed: 1\":\"Close\"})\n",
    "merged_df=merged_df.rename(columns={\"Unnamed: 2\":\"High\"})\n",
    "merged_df=merged_df.rename(columns={\"Unnamed: 3\":\"Low\"})\n",
    "merged_df=merged_df.rename(columns={\"Unnamed: 4\":\"Open\"})\n",
    "merged_df=merged_df.rename(columns={\"Unnamed: 5\":\"Volume\"})\n",
    "\n",
    "merged_df.to_csv(\"merged_midterm_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9535f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7 : Plotting Various Insights from Merged Data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "merged_df=pd.read_csv('merged_midterm_data.csv')\n",
    "news_per_day=merged_df.groupby('date').size()\n",
    "plt.figure()\n",
    "news_per_day.plot(kind=\"line\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of News Articles\")\n",
    "plt.title(\"Daily News Article Count Over Time\")\n",
    "plt.show()\n",
    "news_by_source = merged_df[\"source\"].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "news_by_source.plot(kind=\"bar\")\n",
    "plt.xlabel(\"News Source\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.title(\"News Distribution Across Sources\")\n",
    "plt.show()\n",
    "trading_vs_non = merged_df[\"is_trading_day\"].value_counts()\n",
    "\n",
    "plt.figure()\n",
    "trading_vs_non.plot(kind=\"bar\")\n",
    "plt.xlabel(\"Is Trading Day\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.title(\"Trading vs Non-Trading Day News Frequency\")\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
